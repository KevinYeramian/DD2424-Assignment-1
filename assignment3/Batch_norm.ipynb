{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import for the Assigment3\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import copy as copy\n",
    "from numpy import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variable\n",
    "training_file = \"data_batch_1\"\n",
    "validation_file = \"data_batch_2\"\n",
    "test_file = \"test_batch\"\n",
    "data_folder = \"../../save/assigment1-save/Datasets/cifar-10-batches-py/\"\n",
    "N = 10000\n",
    "K = 10\n",
    "d = 3072\n",
    "m = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasnform a vector of label to a vector of hot-one(dumies)\n",
    "def hot_one(y):\n",
    "    Y = []\n",
    "    for yi in y:\n",
    "        yihot = [0] * K\n",
    "        yihot[yi] = 1\n",
    "        Y.append(yihot)\n",
    "    return np.array(Y)\n",
    "    \n",
    "    \n",
    "#Functions that load data from file.\n",
    "def load_batch(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    y = np.hstack(dict[b'labels'])\n",
    "    X = dict[b'data'].astype(float).T / 255.0\n",
    "    Y = hot_one(y).T\n",
    "    return X,Y,y\n",
    "\n",
    "#Function that normalize the data( Only mean)\n",
    "#Input : - X(d,N)\n",
    "#Output: - X(d,N)\n",
    "\n",
    "def normalizeData(X):\n",
    "    m = np.mean(X,1)\n",
    "    std = np.std(X,1)\n",
    "    #X = (X.T - m).T\n",
    "    return X, m , std\n",
    "\n",
    "def plot_curve(lostTrain, lostVal):\n",
    "    rr = range(len(lostVal))\n",
    "    plt.plot(rr, lostVal,'green')\n",
    "    plt.plot(rr, lostTrain,'blue')\n",
    "    plt.title(\"loss during the training for eta = \" + str(eta) + \" lambda = \" + str(lamb))\n",
    "    green_patch = mpatches.Patch(color='blue', label=\"Training loss\")\n",
    "    blue_patch = mpatches.Patch(color='green', label=\"Validation loss\")\n",
    "    plt.legend(handles=[green_patch, blue_patch])\n",
    "    plt.savefig(str(lamb) + '_'+ str(eta) + '.png')\n",
    "    plt.show()\n",
    "    \n",
    "def initializeWb_Hu(dim_list):\n",
    "    mu = 0\n",
    "    W = []\n",
    "    b = []\n",
    "    for i in range(len(dim_list) - 1):\n",
    "        W.append(np.random.normal(mu, np.sqrt(2.0 / dim_list[i + 1]), (dim_list[i + 1], dim_list[i])))\n",
    "        b.append(np.random.normal(mu, np.sqrt(2.0 / dim_list[i + 1]), (dim_list[i + 1], 1)))\n",
    "    return W, b\n",
    "def initializeWb_Random(dim_list):\n",
    "    mu = 0\n",
    "    W = []\n",
    "    b = []\n",
    "    for i in range(len(dim_list) - 1):\n",
    "        W.append(np.random.normal(mu, 0.1, (dim_list[i + 1], dim_list[i])))\n",
    "        b.append(np.random.normal(mu, 0.1, (dim_list[i + 1], 1)))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load of every datasete\n",
    "X_train, Y_train, y_train = load_batch(data_folder + training_file)\n",
    "X_test, Y_test, y_test = load_batch(data_folder + test_file)\n",
    "X_val, Y_val, y_val = load_batch(data_folder + validation_file)\n",
    "\n",
    "X_train, m, std = normalizeData(X_train)\n",
    "#X_test = (X_test.T - m).T\n",
    "#X_val = (X_val.T - m).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(sll, mul, nul, eps):\n",
    "    assert(len(nul) == len(mul))\n",
    "    sl = copy.deepcopy(sll.T)\n",
    "    for i in range(len(sl)):\n",
    "        sl[i] = (sl[i] - mul) * ((nul + eps) ** -0.5)\n",
    "        assert(len(sl[i]) == len(mul))\n",
    "    return  sl.T\n",
    "\n",
    "\n",
    "def evaluate_Classifier_batch(X, W, b, eps, mean_layer=None, std_layer=None):\n",
    "    X_i = []\n",
    "    h = X\n",
    "    X_i.append(copy.deepcopy(h))\n",
    "    X_i_hat = []\n",
    "    mean_layer_out = []\n",
    "    std_layer_out = []\n",
    "    X_i_hat.append(copy.deepcopy(h))\n",
    "    \n",
    "    for i in range(0,len(W) - 1):\n",
    "  \n",
    "        s1 = np.dot(W[i], h) + b[i]\n",
    "        X_i.append(copy.deepcopy(s1))\n",
    "        if(mean_layer == None): # train\n",
    "            mul = np.mean(s1, 1)\n",
    "            nul = np.std(s1, 1)\n",
    "            mean_layer_out.append(mul)\n",
    "            std_layer_out.append(nul)\n",
    "            s_norm = batch_norm(s1, mul, nul, eps)\n",
    "        else:\n",
    "            print(s1)\n",
    "            print(eps)\n",
    "            s_norm = batch_norm(s1, mean_layer[i], std_layer[i], eps)\n",
    "        h = np.maximum(s_norm, 0)\n",
    "        X_i_hat.append(copy.deepcopy(s_norm))\n",
    "\n",
    "    s = np.dot(W[-1], h) + b[-1]\n",
    "    #X_i.append(copy.deepcopy(s))\n",
    "    aux = np.exp(s)\n",
    "    summ = np.sum(aux, axis=1) #axis=0 sum by colum so we got 1000 therm to normalize\n",
    "    p = aux / summ\n",
    "    return p, X_i, X_i_hat, mean_layer_out, std_layer_out\n",
    "def compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer=None, std_layer=None, p=None):\n",
    "    J = 0.0\n",
    "    if(np.all(p == None)):\n",
    "        p, _, _, _, _ = evaluate_Classifier_batch(X, W, b, eps, mean_layer, std_layer)\n",
    "    for index in range(X.shape[1]):\n",
    "        J += - np.log(np.dot(p[:,index], Y[:,index]))\n",
    "    J /= X.shape[1]\n",
    "    for w in W:\n",
    "        J += lamb * np.sum(np.square(w))\n",
    "    return J\n",
    "    \n",
    "def compute_Accuracy_batch(X, y, W, b, eps, mean_layer=None, std_layer=None, p=None):\n",
    "    if(np.all(p == None)):\n",
    "        p, _, _, _, _= evaluate_Classifier_batch(X, W, b, eps, mean_layer, std_layer) \n",
    "    pred = np.argmax(p,axis=0)\n",
    "    acc = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        if(pred[i] == y[i]):\n",
    "            acc += 1\n",
    "    return acc / len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Gradients_Num_batch(X, Y, P, W, b, lamb, h, eps, mean_layer=None, std_layer=None):\n",
    "    print(W[0].shape)\n",
    "    grad_W = []\n",
    "    grad_b = []\n",
    "    N = X.shape[1]\n",
    "    # Initialization of grad\n",
    "    for i in range(len(W)):\n",
    "        grad_W.append(np.zeros((W[i].shape[0],W[i].shape[1])))\n",
    "        grad_b.append(np.zeros((b[i].shape[0],b[i].shape[1])))\n",
    "\n",
    "    c =  compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer, std_layer)\n",
    "    #print(c)\n",
    "    for index in range(len(W)):\n",
    "        for i in range(W[index].shape[0]):\n",
    "            for j in range(W[index].shape[1]):\n",
    "                W[index][i][j] += h\n",
    "                c2 = compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer, std_layer)\n",
    "                W[index][i][j] -= 2 * h\n",
    "                c1 = compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer, std_layer)\n",
    "                grad_W[index][i][j] = (c2 - c1) / (2 * h)\n",
    "                W[index][i][j] += h\n",
    "\n",
    "        for i in range(len(b[index])):\n",
    "            b[index][i] += h\n",
    "            c2 = compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer, std_layer)\n",
    "            b[index][i] -= 2 * h\n",
    "            c1 = compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer, std_layer)\n",
    "            b[index][i] += h\n",
    "            grad_b[index][i] = (c2 - c1) / (2 * h)\n",
    "            \n",
    "                \n",
    "   \n",
    "    return np.array(grad_W), np.array(grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_Gradients_batch(X, Y, p, X_i, X_i_hat, W, b, lamb, mean_layer, std_layer):\n",
    "    grad_W = []\n",
    "    grad_b = []\n",
    "    N = X.shape[1]\n",
    "    #print(N)\n",
    "    # Initialization of grad\n",
    "    for i in range(len(W)):\n",
    "        grad_W.append(np.zeros((W[i].shape[0],W[i].shape[1])))\n",
    "        grad_b.append(np.zeros((b[i].shape[0],b[i].shape[1])))\n",
    "    \n",
    "    g = - (Y.T - p.T).T\n",
    "    \n",
    "    gx = np.dot(g, np.maximum(X_i_hat[-1].T, 0))\n",
    "    \n",
    "    grad_b[-1] += np.mean(g, axis=-1, keepdims=True)\n",
    "    grad_W[-1] += gx\n",
    "    grad_W[-1] /= N\n",
    "    grad_W[-1] += 2 * lamb * W[-1]\n",
    "    \n",
    "    #First update of g\n",
    "    \n",
    "    g = np.dot(g.T,W[-1])\n",
    "    g1 = np.where(X_i_hat[-1] > 0, 1, 0) \n",
    "    g = np.multiply(g.T, g1)\n",
    "    for l in range(len(W) - 2, -1, -1):\n",
    "        g = g.T\n",
    "        X_i_centered = np.subtract(X_i[l + 1].T, mean_layer[l])\n",
    "        \n",
    "        gradJ_std = - (std_layer[l] ** (-1.5)) * np.sum(g * X_i_centered, axis=1, keepdims=True)\n",
    "        gradJ_mean = - (std_layer[l] ** (-0.5)) * np.sum(g, axis=1, keepdims=True)\n",
    "        g =  g * (std_layer[l] ** (-0.5)) + (X_i_centered * gradJ_std) / N #+ (gradJ_mean / N)\n",
    "         \n",
    "        g = g.T\n",
    "        grad_b[l] += np.mean(g, axis=1, keepdims=True)\n",
    "        \n",
    "        gx = np.dot(g,  np.maximum(X_i_hat[l].T, 0))\n",
    "        grad_W[l] += gx\n",
    "        grad_W[l] /= N\n",
    "        grad_W[l] += 2 * lamb * W[l]\n",
    "        \n",
    "        g = np.dot(g.T,W[l])\n",
    "        g1 = np.where(X_i_hat[l] > 0, 1, 0)\n",
    "        g = np.multiply(g.T, g1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return grad_W, grad_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-82fa74680d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-06\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mW_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializeWb_Hu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "d = 5\n",
    "n = 10\n",
    "dim_list = [d, 20, 5, 7,K]\n",
    "h = 1e-4\n",
    "lamb = 0\n",
    "rho = 0.9\n",
    "\n",
    "eps = 1e-06\n",
    "\n",
    "X = X_train[:d,:n]\n",
    "Y = Y_train[:,:n]\n",
    "W_init, b_init = initializeWb_Hu(dim_list)\n",
    "\n",
    "p, X_i,X_i_hat, mean_layer, std_layer  = evaluate_Classifier_batch(X, W_init, b_init , eps)\n",
    "\n",
    "grad_W11, grad_b11  = compute_Gradients_Num_batch(X, Y, p, W_init, b_init, lamb, h, eps)\n",
    "grad_W1, grad_b1 = compute_Gradients_batch(X, Y, p, X_i, X_i_hat, W_init, b_init, lamb, mean_layer, std_layer)\n",
    "\n",
    "for i in range(len(W_init)):\n",
    "    print(\"Norm for W & b for the \" + str(i) + \"layer\")\n",
    "    assert(grad_W1[i].shape ==grad_W11[i].shape)\n",
    "    print(np.linalg.norm(grad_W1[i] - grad_W11[i],ord=1) / max(h ,np.linalg.norm(grad_W1[i] ,ord=1) + np.linalg.norm(grad_W11[i] ,ord=1)))\n",
    "    print(np.linalg.norm(grad_b1[i] - grad_b11[i],ord=1) / max(h ,np.linalg.norm(grad_b1[i] ,ord=1) + np.linalg.norm(grad_b11[i] ,ord=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]]\n",
      "[[-0.12060852]\n",
      " [-0.10509788]]\n"
     ]
    }
   ],
   "source": [
    "print(grad_b11[0])\n",
    "print(grad_b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, y, X_val, Y_val, y_val, lamb, n_batch, rho, eta , n_epochs, dim_list, W, b, eps, norm_coef):\n",
    "    d = X.shape[0]\n",
    "    N = X.shape[1]\n",
    "    print(W[0].shape)\n",
    "    print(b[0].shape)\n",
    "    lostTrain = []\n",
    "    lostVal = []\n",
    "    W_momentum = []\n",
    "    b_momentum = []\n",
    "    for i in range(len(W)):\n",
    "        W_momentum.append(np.zeros((W[i].shape[0],W[i].shape[1])))\n",
    "        b_momentum.append(np.zeros((b[i].shape[0],b[i].shape[1])))\n",
    "    mean_layer=None\n",
    "    std_layer=None\n",
    "    for _ in range(n_epochs):\n",
    "        \n",
    "        #print(random.permutation(range(X.shape[1])))\n",
    "        for j in range(int(N / n_batch)):\n",
    "            print('j' , j)\n",
    "            start = j * n_batch\n",
    "            end = (j + 1) * n_batch\n",
    "            X_batch = X[:,start:end]\n",
    "            Y_batch = Y[:,start:end]\n",
    "\n",
    "            p, X_i, X_i_hat, mean_layer_out, std_layer_out = evaluate_Classifier_batch(X_batch, W, b, eps)\n",
    "            #print(compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer_out, std_layer_out))\n",
    "            #print(mean_layer_out)\n",
    "\n",
    "            if(mean_layer == None or std_layer == None):\n",
    "                mean_layer = copy.deepcopy(mean_layer_out)\n",
    "                std_layer = copy.deepcopy(std_layer_out)\n",
    "\n",
    "            grad_W, grad_b = compute_Gradients_Num_batch(X_batch, Y_batch, p, W, b, lamb, 10e-04,mean_layer_out, std_layer_out)\n",
    "            #(X, Y, P, W, b, lamb, h, eps, mean_layer=None, std_layer=None)\n",
    "            for i in range(len(W) - 1):\n",
    "                mean_layer[i] = norm_coef *  mean_layer[i] + (1 - norm_coef) * mean_layer_out[i]\n",
    "                std_layer[i] = norm_coef *  std_layer[i] + (1 - norm_coef) * std_layer_out[i]\n",
    "            for i in range(len(W)):\n",
    "                W_momentum[i] = W_momentum[i] * rho +  eta * grad_W[i]\n",
    "                b_momentum[i] = b_momentum[i] * rho +  eta * grad_b[i]\n",
    "                W[i] -= W_momentum[i]\n",
    "                b[i] -= b_momentum[i]\n",
    "            \n",
    "    \n",
    "        #print(\"Accurency on the test = \"  + str(compute_Accuracy(X_val, y_val, W1, b1, W2, b2)))\n",
    "        eta *= 0.95\n",
    "        lostTrain.append(compute_Cost_batch(X, Y, W, b, lamb, eps, mean_layer, std_layer))\n",
    "        lostVal.append(compute_Cost_batch(X_val, Y_val, W, b, lamb,eps, mean_layer, std_layer))\n",
    "    \n",
    "    return W, b, lostTrain, lostVal, mean_layer, std_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3072)\n",
      "6261.054867000741\n",
      "(50, 3072)\n",
      "(50, 3072)\n",
      "(50, 1)\n",
      "j 0\n",
      "(50, 3072)\n",
      "[[ -5.3927631  -12.92601753 -16.09839006 ...  -7.86558571  -8.04834793\n",
      "   -6.98957956]\n",
      " [-10.12496842 -10.36238966 -10.47982904 ... -13.42780786  -9.19623965\n",
      "   -5.64500465]\n",
      " [  1.74481338   2.60517613   5.159706   ...   4.44739827   3.38768928\n",
      "    2.8023769 ]\n",
      " ...\n",
      " [  1.25116503   2.46917663   4.02869836 ...   0.84178926   1.30899796\n",
      "   -3.54902262]\n",
      " [ -0.24692414  -2.65893861  -6.33807498 ...  -1.44650468  -3.10967696\n",
      "   -1.0497437 ]\n",
      " [  0.46845644  -3.7678704    2.14473209 ...  -1.45760971  -1.86226212\n",
      "   -0.89628707]]\n",
      "[array([ -9.35939626, -10.17967869,   3.97193435,   5.47473238,\n",
      "         1.90514249,  -1.25578804,   6.3701574 ,   1.56112871,\n",
      "        -2.53242901,  -0.02466872,  -2.22653978,   0.08903954,\n",
      "         6.36399711,   5.36357642,  -4.40384424,  -5.78639415,\n",
      "         3.66692146,   2.99143764,  -5.78532425,   2.96855765,\n",
      "        -9.25923465,  -8.58961413,  -1.17402286,  -4.75367658,\n",
      "        -2.4749522 ,  12.328857  ,  -2.98298938,  -5.48786054,\n",
      "        -3.99239736,  -2.4944582 ,   5.32991783,  -4.74046922,\n",
      "        -2.62204928,  12.8558227 ,  -4.14865209, -11.37563263,\n",
      "        -6.52360992,  -0.42064536,   3.63441237,   3.42151728,\n",
      "         0.75801756,   0.54123859,   7.13601387,  -5.38944314,\n",
      "         4.16286152,  -5.51044868,  -6.33962997,   1.02933405,\n",
      "        -2.05135224,  -2.15920107])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-cb11b36b2959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_Cost_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlostTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlostVal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accurency on the test = \"\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_Accuracy_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-365-ea4ef9084bf0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X, Y, y, X_val, Y_val, y_val, lamb, n_batch, rho, eta, n_epochs, dim_list, W, b, eps, norm_coef)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mstd_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_layer_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_Gradients_Num_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10e-04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_layer_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_layer_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m#(X, Y, P, W, b, lamb, h, eps, mean_layer=None, std_layer=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-368-9d926070c305>\u001b[0m in \u001b[0;36mcompute_Gradients_Num_batch\u001b[0;34m(X, Y, P, W, b, lamb, h, eps, mean_layer, std_layer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgrad_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mcompute_Cost_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-367-29b26b34f524>\u001b[0m in \u001b[0;36mcompute_Cost_batch\u001b[0;34m(X, Y, W, b, lamb, eps, mean_layer, std_layer, p)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_Classifier_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mJ\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-367-29b26b34f524>\u001b[0m in \u001b[0;36mevaluate_Classifier_batch\u001b[0;34m(X, W, b, eps, mean_layer, std_layer)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0ms_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mX_i_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "N_bis = 9000\n",
    "K = 10\n",
    "d = 3072\n",
    "lamb = 1\n",
    "\n",
    "dim_list = [d, 50, K]\n",
    "X = X_train[:,:N_bis]\n",
    "Y = Y_train[:,:N_bis]\n",
    "y = y_train[:N_bis]\n",
    "\n",
    "X_val = X_train[:,N_bis:]\n",
    "Y_val = Y_train[:,N_bis:]\n",
    "y_val = y_train[N_bis:]\n",
    "\n",
    "\n",
    "n_batch= 100\n",
    "eta= 0.01\n",
    "n_epochs= 5\n",
    "rho = 0.9\n",
    "W_init, b_init = initializeWb_Hu(dim_list)\n",
    "eps = 10e-06\n",
    "print(W_init[0].shape)\n",
    "print(compute_Cost_batch(X, Y, W_init, b_init, lamb,eps, None, None))\n",
    "print(W_init[0].shape)\n",
    "W, b, lostTrain, lostVal, mean_layer, std_layer = fit(X, Y, y, X_val, Y_val, y_val,  lamb, n_batch, rho, eta, n_epochs, dim_list, W_init, b_init, eps, 0.9)\n",
    "\n",
    "print(\"Accurency on the test = \"  + str(compute_Accuracy_batch(X_test, y_test, W, b, eps, mean_layer, std_layer)))\n",
    "plot_curve(lostTrain, lostVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurency on the test = 0.0898\n",
      "Cost function = 2.3704239139224574\n",
      "One proba : [[0.09520598 0.08502072 0.05515778 ... 0.08454912 0.06930327 0.06572916]\n",
      " [0.1307071  0.10956342 0.11762471 ... 0.12731229 0.14743798 0.08838231]\n",
      " [0.07880121 0.08726361 0.11994545 ... 0.09882183 0.12510053 0.06707362]\n",
      " ...\n",
      " [0.13037742 0.14061573 0.21344733 ... 0.15510613 0.19945646 0.14954028]\n",
      " [0.10591839 0.10555981 0.08052146 ... 0.09205797 0.06701409 0.11996603]\n",
      " [0.0981124  0.083172   0.06707408 ... 0.08261487 0.0567159  0.07901497]]\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "N_bis = 1000\n",
    "K = 10\n",
    "d = 3072\n",
    "lamb = 0\n",
    "eps = 10e-06\n",
    "dim_list = [d,10, K]\n",
    "W, b = initializeWb_Random(dim_list)\n",
    "X = X_train[:,:N_bis]\n",
    "Y = Y_train[:,:N_bis]\n",
    "y = y_train[:N_bis]\n",
    "X_val = X_train[:,N_bis:]\n",
    "Y_val = Y_train[:,N_bis:]\n",
    "y_val = y_train[N_bis:]\n",
    "\n",
    "n_batch= 100\n",
    "eta= 0.0252\n",
    "n_epochs= 10\n",
    "rho = 0.9\n",
    "print(\"Accurency on the test = \"  + str(compute_Accuracy_batch(X_train, y_train, W, b, eps)))\n",
    "print(\"Cost function = \" + str(compute_Cost_batch(X, Y, W, b, lamb, eps)))\n",
    "print(\"One proba : \" + str(evaluate_Classifier_batch(X, W, b, eps)[0][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
